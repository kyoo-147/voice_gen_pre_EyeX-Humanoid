{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forward Tacotron PyTorch-TFLite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoIZmFLzymsB"
      },
      "source": [
        "This notebook converts Forward Tacotron pre-trained PyTorch Model to ONNX. In future it will be updated to support TFLite Conversion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeuGGIloy6lg"
      },
      "source": [
        "## Acknowledgments\n",
        "\n",
        "- Pre Trained Model is taken from [Transformer TTS Repository](https://github.com/as-ideas/TransformerTTS) by Axel Springer.\n",
        "- Model Utilities and helper functions are also taken from the same [repository](https://github.com/as-ideas/TransformerTTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIIZ2RnesZTw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHfPLGkqe-Jx"
      },
      "source": [
        "# Clone the repo including pretrained models\n",
        "!git clone https://github.com/as-ideas/ForwardTacotron.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mozuFkefE41"
      },
      "source": [
        "# Install requirements\n",
        "%cd ForwardTacotron/  \n",
        "!apt-get install espeak\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEE-B9LIzZZZ"
      },
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install pip install git+https://github.com/onnx/onnx-tensorflow.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6vTAaysucsY"
      },
      "source": [
        "## Download Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6FFe4g2fuBZ"
      },
      "source": [
        "# Load pretrained models\n",
        "from pathlib import Path\n",
        "from typing import Union, Callable, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from models.tacotron import CBHG\n",
        "from utils.text import text_to_sequence, clean_text\n",
        "from utils.text.symbols import phonemes\n",
        "from utils import hparams as hp\n",
        "\n",
        "from notebook_utils.synthesize import (\n",
        "    get_forward_model, get_melgan_model, get_wavernn_model, synthesize, init_hparams)\n",
        "from utils import hparams as hp\n",
        "import IPython.display as ipd\n",
        "init_hparams('pretrained/pretrained_hparams.py')\n",
        "voc_melgan = get_melgan_model() \n",
        "voc_wavernn = get_wavernn_model('pretrained/wave_575K.pyt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWTCDpesido2",
        "cellView": "form"
      },
      "source": [
        "#@title Model Helper Functions\n",
        "\n",
        "pitch_function: Callable[[torch.tensor], torch.tensor] = lambda x: x\n",
        "\n",
        "class LengthRegulator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dur):\n",
        "        return self.expand(x, dur)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_index(duration, x):\n",
        "        duration[duration < 0] = 0\n",
        "        tot_duration = duration.cumsum(1).detach().cpu().numpy().astype('int')\n",
        "        max_duration = int(tot_duration.max().item())\n",
        "        index = np.zeros([x.shape[0], max_duration, x.shape[2]], dtype='long')\n",
        "\n",
        "        for i in range(tot_duration.shape[0]):\n",
        "            pos = 0\n",
        "            for j in range(tot_duration.shape[1]):\n",
        "                pos1 = tot_duration[i, j]\n",
        "                index[i, pos:pos1, :] = j\n",
        "                pos = pos1\n",
        "            index[i, pos:, :] = j\n",
        "        return torch.LongTensor(index).to(duration.device)\n",
        "\n",
        "    def expand(self, x, dur):\n",
        "        idx = self.build_index(dur, x)\n",
        "        y = torch.gather(x, 1, idx)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SeriesPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dims, conv_dims=256, rnn_dims=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList([\n",
        "            BatchNormConv(in_dims, conv_dims, 5, activation=torch.relu),\n",
        "            BatchNormConv(conv_dims, conv_dims, 5, activation=torch.relu),\n",
        "            BatchNormConv(conv_dims, conv_dims, 5, activation=torch.relu),\n",
        "        ])\n",
        "        self.rnn = nn.GRU(conv_dims, rnn_dims, batch_first=True, bidirectional=True)\n",
        "        self.lin = nn.Linear(2 * rnn_dims, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, alpha=1.0):\n",
        "        x = x.transpose(1, 2)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = x.transpose(1, 2)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.lin(x)\n",
        "        return x / alpha\n",
        "\n",
        "\n",
        "class ConvResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dims, conv_dims=256):\n",
        "        super().__init__()\n",
        "        self.first_conv = BatchNormConv(in_dims, conv_dims, 5, activation=torch.relu)\n",
        "        self.convs = torch.nn.ModuleList([\n",
        "            BatchNormConv(conv_dims, conv_dims, 5, activation=torch.relu),\n",
        "            BatchNormConv(conv_dims, conv_dims, 5, activation=torch.relu),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.first_conv(x)\n",
        "        for conv in self.convs:\n",
        "            x_res = x\n",
        "            x = conv(x)\n",
        "            x = x_res + x\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class BatchNormConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, activation=None):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel, stride=1, padding=kernel // 2, bias=False)\n",
        "        self.bnorm = nn.BatchNorm1d(out_channels)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        x = self.bnorm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ForwardTacotron(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dims,\n",
        "                 num_chars,\n",
        "                 durpred_conv_dims,\n",
        "                 durpred_rnn_dims,\n",
        "                 durpred_dropout,\n",
        "                 pitch_conv_dims,\n",
        "                 pitch_rnn_dims,\n",
        "                 pitch_dropout,\n",
        "                 pitch_emb_dims,\n",
        "                 pitch_proj_dropout,\n",
        "                 rnn_dim,\n",
        "                 prenet_k,\n",
        "                 prenet_dims,\n",
        "                 postnet_k,\n",
        "                 postnet_dims,\n",
        "                 highways,\n",
        "                 dropout,\n",
        "                 n_mels):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.embedding = nn.Embedding(num_chars, embed_dims)\n",
        "        self.lr = LengthRegulator()\n",
        "        self.dur_pred = SeriesPredictor(embed_dims,\n",
        "                                        conv_dims=durpred_conv_dims,\n",
        "                                        rnn_dims=durpred_rnn_dims,\n",
        "                                        dropout=durpred_dropout)\n",
        "        self.pitch_pred = SeriesPredictor(embed_dims,\n",
        "                                          conv_dims=pitch_conv_dims,\n",
        "                                          rnn_dims=pitch_rnn_dims,\n",
        "                                          dropout=pitch_dropout)\n",
        "        self.prenet = CBHG(K=prenet_k,\n",
        "                           in_channels=embed_dims,\n",
        "                           channels=prenet_dims,\n",
        "                           proj_channels=[prenet_dims, embed_dims],\n",
        "                           num_highways=highways)\n",
        "        self.lstm = nn.LSTM(2 * prenet_dims + pitch_emb_dims,\n",
        "                            rnn_dim,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.lin = torch.nn.Linear(2 * rnn_dim, n_mels)\n",
        "        self.register_buffer('step', torch.zeros(1, dtype=torch.long))\n",
        "        self.postnet = CBHG(K=postnet_k,\n",
        "                            in_channels=n_mels,\n",
        "                            channels=postnet_dims,\n",
        "                            proj_channels=[postnet_dims, n_mels],\n",
        "                            num_highways=highways)\n",
        "        self.dropout = dropout\n",
        "        self.post_proj = nn.Linear(2 * postnet_dims, n_mels, bias=False)\n",
        "        self.pitch_emb_dims = pitch_emb_dims\n",
        "        if pitch_emb_dims > 0:\n",
        "            self.pitch_proj = nn.Sequential(\n",
        "                nn.Conv1d(1, pitch_emb_dims, kernel_size=3, padding=1),\n",
        "                nn.Dropout(pitch_proj_dropout))\n",
        "\n",
        "    def forward(self,\n",
        "                 x: List[int],\n",
        "                 alpha=1.0,\n",
        "                 pitch_function: Callable[[torch.tensor], torch.tensor] = lambda x: x) -> tuple:\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device  # use same device as parameters\n",
        "        x = torch.as_tensor(x, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        dur = self.dur_pred(x, alpha=alpha)\n",
        "        dur = dur.squeeze(2)\n",
        "\n",
        "        pitch_hat = self.pitch_pred(x).transpose(1, 2)\n",
        "        pitch_hat = pitch_function(pitch_hat)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.prenet(x)\n",
        "\n",
        "        if self.pitch_emb_dims > 0:\n",
        "            pitch_hat_proj = self.pitch_proj(pitch_hat).transpose(1, 2)\n",
        "            x = torch.cat([x, pitch_hat_proj], dim=-1)\n",
        "\n",
        "        x = self.lr(x, dur)\n",
        "\n",
        "        x, _ = self.lstm(x)\n",
        "        x = F.dropout(x,\n",
        "                      p=self.dropout,\n",
        "                      training=self.training)\n",
        "        x = self.lin(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x_post = self.postnet(x)\n",
        "        x_post = self.post_proj(x_post)\n",
        "        x_post = x_post.transpose(1, 2)\n",
        "\n",
        "        x, x_post, dur = x.squeeze(), x_post.squeeze(), dur.squeeze()\n",
        "        # x = x.cpu().data.numpy()\n",
        "        # x_post = x_post.cpu().data.numpy()\n",
        "        # dur = dur.cpu().data.numpy()\n",
        "\n",
        "        return x, x_post, pitch_hat\n",
        "\n",
        "    def pad(self, x, max_len):\n",
        "        x = x[:, :, :max_len]\n",
        "        x = F.pad(x, [0, max_len - x.size(2), 0, 0], 'constant', -11.5129)\n",
        "        return x\n",
        "\n",
        "    def get_step(self):\n",
        "        return self.step.data.item()\n",
        "\n",
        "    def load(self, path: Union[str, Path]):\n",
        "        # Use device of model params as location for loaded state\n",
        "        device = next(self.parameters()).device\n",
        "        state_dict = torch.load(path, map_location=device)\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def save(self, path: Union[str, Path]):\n",
        "        # No optimizer argument because saving a model should not include data\n",
        "        # only relevant in the training process - it should only be properties\n",
        "        # of the model itself. Let caller take care of saving optimzier state.\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def log(self, path, msg):\n",
        "        with open(path, 'a') as f:\n",
        "            print(msg, file=f)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FAJDqRBj7Bl"
      },
      "source": [
        "def get_forward_model(model_path):\n",
        "    device = torch.device('cuda')\n",
        "    model = ForwardTacotron(embed_dims=hp.forward_embed_dims,\n",
        "                            num_chars=len(phonemes),\n",
        "                            durpred_rnn_dims=hp.forward_durpred_rnn_dims,\n",
        "                            durpred_conv_dims=hp.forward_durpred_conv_dims,\n",
        "                            durpred_dropout=hp.forward_durpred_dropout,\n",
        "                            pitch_rnn_dims=hp.forward_pitch_rnn_dims,\n",
        "                            pitch_conv_dims=hp.forward_pitch_conv_dims,\n",
        "                            pitch_dropout=hp.forward_pitch_dropout,\n",
        "                            pitch_emb_dims=hp.forward_pitch_emb_dims,\n",
        "                            pitch_proj_dropout=hp.forward_pitch_proj_dropout,\n",
        "                            rnn_dim=hp.forward_rnn_dims,\n",
        "                            postnet_k=hp.forward_postnet_K,\n",
        "                            postnet_dims=hp.forward_postnet_dims,\n",
        "                            prenet_k=hp.forward_prenet_K,\n",
        "                            prenet_dims=hp.forward_prenet_dims,\n",
        "                            highways=hp.forward_num_highways,\n",
        "                            dropout=hp.forward_dropout,\n",
        "                            n_mels=hp.num_mels).to(device)\n",
        "    model.load(model_path)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ5JTJ-SxxYR"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO3Evvntj92b"
      },
      "source": [
        "tts_model = get_forward_model('pretrained/forward_46K.pyt')\n",
        "\n",
        "tts_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jwuf6hPhCS3",
        "outputId": "e2527a2a-e1d1-4eb1-ecab-ff2da0498ad2"
      },
      "source": [
        "input_text = 'Checking the quality of Forward Tacotorn'\n",
        "\n",
        "text = clean_text(input_text.strip())\n",
        "x = text_to_sequence(text)\n",
        "x = np.asarray(x)\n",
        "x = torch.from_numpy(x)\n",
        "x.size()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([39])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EBNxK4Zx1V6"
      },
      "source": [
        "## Export to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwangBDkgx8H"
      },
      "source": [
        "torch.onnx.export(tts_model,               # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"forward_tac.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=12,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output1', 'output2',\n",
        "                                  'output4'], # the model's output names\n",
        "                  dynamic_axes={'input' : {1 : 'seq_length'},    # variable lenght axes\n",
        "                                'output2' : {1 : 'seq_length'}})\n",
        "print(\"Model converted succesfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdjynb8Mx6NP"
      },
      "source": [
        "## ONNX Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqHdeUcJkqnX"
      },
      "source": [
        "import onnxruntime\n",
        "\n",
        "onnx_runtime_input = x.detach().numpy()\n",
        "ort_session = onnxruntime.InferenceSession(\"forward_tac.onnx\")\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    print(tensor)\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "# # compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: onnx_runtime_input}\n",
        "ort_outs = ort_session.run(None, ort_inputs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0agER4IyPRT"
      },
      "source": [
        "## Synthesize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OOpXhFDyRYA"
      },
      "source": [
        "#### ONNX Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDrDWzlyFyp"
      },
      "source": [
        "m = torch.tensor(ort_outs[1]).unsqueeze(0).cuda()\n",
        "with torch.no_grad():\n",
        "    wav = voc_melgan.inference(m).cpu().numpy()\n",
        "ipd.Audio(wav, rate=hp.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H50k_-zhyU8K"
      },
      "source": [
        "#### PyTorch Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2e6e6KcoeAu"
      },
      "source": [
        "# Run PyTorch Model\n",
        "_, torch_out, _ = tts_model(x, alpha=1)\n",
        "# Synthesize with melgan and PyTorch Model Output\n",
        "torch_out = torch_out.unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    wav = vocoder.inference(torch_out).cpu().detach().numpy()\n",
        "ipd.Audio(wav, rate=hp.sample_rate)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rSL8lDTyYF4"
      },
      "source": [
        "## Compare PyTorch and ONNX Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnmMSIkCobzE"
      },
      "source": [
        "np.testing.assert_allclose(m, ort_outs[1], rtol=1e-03, atol=1e-04)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LANmkuXzuPor"
      },
      "source": [
        "# import onnx\n",
        "# from onnx_tf.backend import prepare\n",
        "\n",
        "# onnx_model = onnx.load('forward_tac.onnx')\n",
        "# tf_rep = prepare(onnx_model)\n",
        "# tf_rep.export_graph('forward_tac.pb')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}